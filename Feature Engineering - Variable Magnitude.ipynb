{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Variable Magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the magnitude of the variable matter?\n",
    "In Linear Regression models, the scale of variables used to estimate the output matters. Linear models are of the type y = w x + b, where the regression coefficient w represents the expected change in y for a one unit change in x (the predictor). Thus, the magnitude of w is partly determined by the magnitude of the units being used for x. If x is a distance variable, just changing the scale from kilometers to miles will cause a change in the magnitude of the coefficient.\n",
    "\n",
    "In addition, in situations where we estimate the outcome y by contemplating multiple predictors x1, x2, \u2026xn, predictors with greater numeric ranges dominate over those with smaller numeric ranges.\n",
    "\n",
    "Gradient descent converges faster when all the predictors (x1 to xn) are within a similar scale, therefore having features in a similar scale is useful for Neural Networks as well as.\n",
    "\n",
    "In Support Vector Machines, feature scaling can decrease the time required to find the support vectors.\n",
    "\n",
    "Finally, methods using Euclidean distances or distances in general are also affected by the magnitude of the features, as Euclidean distance is sensitive to variations in the magnitude or scales of the predictors. Therefore feature scaling is required for methods that utilise distance calculations like k-nearest neighbours (KNN) and k-means clustering.\n",
    "\n",
    "#### In short:\n",
    "\n",
    "Magnitude matters because:\n",
    "\n",
    " - The regression coefficient is directly influenced by the scale of the variable\n",
    " - Variables with bigger magnitude / value range dominate over the ones with smaller magnitude / value range\n",
    " - Gradient descent converges faster when features are on similar scales\n",
    " - Feature scaling helps decrease the time to find support vectors for SVMs\n",
    " - Euclidean distances are sensitive to feature magnitude.\n",
    "\n",
    "#### The machine learning models affected by the magnitude of the feature are:\n",
    " - Linear and Logistic Regression\n",
    " - Neural Networks\n",
    " - Support Vector Machines\n",
    " - KNN\n",
    " - K-means clustering\n",
    " - Linear Discriminant Analysis (LDA)\n",
    " - Principal Component Analysis (PCA)\n",
    "\n",
    "#### Machine learning models insensitive to feature magnitude are the ones based on Trees:\n",
    " - Classification and Regression Trees\n",
    " - Random Forests\n",
    " - Gradient Boosted Trees\n",
    " \n",
    "#### In this Blog\n",
    "We will study the effect of feature magnitude on the performance of different machine learning algorithms.\n",
    "\n",
    "We will use the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read the dataset into a dataframe and perform operations on it\n",
    "# to perform basic array operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "# import several machine learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# to evaluate performance and separate into\n",
    "# train and test set\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# to scale the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data with numerical variables only\n",
    "We will start by loading only the variables having numeric values from the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/titanic.csv\",\n",
    "    usecols=[\"Pclass\", \"Age\", \"Fare\", \"Survived\"],\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have a look at the values of those variables to get an idea of the feature magnitudes. describe provides descriptive statistics including those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Fare varies between 0 and 512, Age between 0 and 80, and Pclass between 1 and 3. So the variables have different magnitudes.\n",
    "\n",
    "Let\u2019s calculate the range of each variable. The range of a set of data is the difference between the largest and smallest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Pclass\", \"Age\", \"Fare\"]:\n",
    "    print(col, \"range: \", data[col].max() - data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values that each variable takes are quite different.\n",
    "\n",
    "Now we will split the data into training and testing set with the help of train_test_split(). We will use the variables Pclass, Age and Fare as the feature space and Survived as the target. The test_size = 0.3 will keep 30% data for testing and 70% data will be used for training the model. random_state controls the shuffling applied to the data before applying the split. The titanic dataset contains missing information so for this demo, we will fill those with 0s using fillna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[[\"Pclass\", \"Age\", \"Fare\"]].fillna(0),\n",
    "    data.Survived,\n",
    "    test_size=0.3,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains 623 rows while the test dataset contains 268 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "For this demonstration, we will scale the features between 0 and 1, using the MinMaxScaler from scikit-learn. To learn more about this scaling visit the Scikit-Learn website\n",
    "\n",
    "The transformation is given by:\n",
    "\n",
    "#### X_rescaled = (X \u2013 X.min) / (X.max \u2013 X.min)\n",
    "\n",
    "And to transform the re-scaled features back to their original magnitude:\n",
    "\n",
    "#### X = X_rescaled * (max \u2013 min) + min\n",
    "\n",
    "We will first initialize scalar. Then we will fit the scalar to the training dataset. Using this scalar we will transform X_train as well as X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the scaler\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# re scale the datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\u2019s have a look at the scaled training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean: \", X_train_scaled.mean(axis=0))\n",
    "print(\"Standard Deviation: \", X_train_scaled.std(axis=0))\n",
    "print(\"Minimum value: \", X_train_scaled.min(axis=0))\n",
    "print(\"Maximum value: \", X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the maximum values for all the features is 1, and the minimum value is zero, as expected. So they are in a similar scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Let\u2019s evaluate the effect of feature scaling on a Logistic Regression. We will first build the model using unscaled variables and then the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built on unscaled variables\n",
    "\n",
    "# call the model\n",
    "logit = LogisticRegression(\n",
    "    random_state=44,\n",
    "    C=1000,  # Inverse of regularization strength (larger c to avoid regularization)\n",
    "    solver=\"lbfgs\",\n",
    ")  # Algorithm to use in the optimization problem.\n",
    "\n",
    "# train the model\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = logit.predict_proba(X_train)\n",
    "print(\"Logistic Regression roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = logit.predict_proba(X_test)\n",
    "print(\"Logistic Regression roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\u2019s look at the coefficients. coef_ gives the coefficient of the features in the decision function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built on scaled variables\n",
    "\n",
    "# call the model\n",
    "logit = LogisticRegression(\n",
    "    random_state=44,\n",
    "    C=1000,  # Inverse of regularization strength (larger c to avoid regularization)\n",
    "    solver=\"lbfgs\",\n",
    ")  # Algorithm to use in the optimization problem.\n",
    "\n",
    "# train the model using the re-scaled data\n",
    "logit.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = logit.predict_proba(X_train_scaled)\n",
    "print(\"Logistic Regression roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = logit.predict_proba(X_test_scaled)\n",
    "print(\"Logistic Regression roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\u2019s look at the coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the performance of logistic regression did not change due to the datasets with the features scaled (compare roc-auc values for train and test set for models with and without feature scaling).\n",
    "\n",
    "However, when looking at the coefficients we do see a big difference in the values. This is because the magnitude of the variable was affecting the coefficients. After scaling, all 3 variables have the relatively the same effect (coefficient) towards survival, whereas before scaling, we would be inclined to think that PClass was driving the Survival outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "Let\u2019s evaluate the effect of feature scaling on Support Vector Machines. We will first build the model using unscaled variables and then the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build on unscaled variables\n",
    "\n",
    "# call the model\n",
    "SVM_model = SVC(random_state=44, probability=True, gamma=\"auto\")\n",
    "\n",
    "#  train the model\n",
    "SVM_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = SVM_model.predict_proba(X_train)\n",
    "print(\"SVM roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = SVM_model.predict_proba(X_test)\n",
    "print(\"SVM roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built on scaled variables\n",
    "\n",
    "# call the model\n",
    "SVM_model = SVC(random_state=44, probability=True, gamma=\"auto\")\n",
    "\n",
    "# train the model\n",
    "SVM_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = SVM_model.predict_proba(X_train_scaled)\n",
    "print(\"SVM roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = SVM_model.predict_proba(X_test_scaled)\n",
    "print(\"SVM roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling improved the performance of the support vector machine. After feature scaling the model is no longer over-fitting to the training set (compare the roc-auc of 0.9 for the model on unscaled features vs the roc-auc of 0.7). In addition, the roc-auc for the testing set increased as well (0.67 vs 0.69).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbours\n",
    "Let\u2019s evaluate the effect of feature scaling on K-Nearest Neighbours. We will first build the model using unscaled variables and then the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built on unscaled features\n",
    "\n",
    "# call the model\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# train the model\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = KNN.predict_proba(X_train)\n",
    "print(\"KNN roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = KNN.predict_proba(X_test)\n",
    "print(\"KNN roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built on scaled\n",
    "\n",
    "# call the model\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# train the model\n",
    "KNN.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = KNN.predict_proba(X_train_scaled)\n",
    "print(\"KNN roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = KNN.predict_proba(X_test_scaled)\n",
    "print(\"KNN roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe for KNN as well that feature scaling improved the performance of the model. The model built on scaled features shows a better generalisation, with a higher roc-auc 0.72 for the testing set vs 0.69 for model built on unscaled features.\n",
    "\n",
    "Both KNN methods are over-fitting to the train set. Thus, we would need to change the parameters of the model or use less features to try and decrease over-fitting, which exceeds the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "Let\u2019s evaluate the effect of feature scaling on Random Forests. We will first build the model using unscaled variables and then the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built on unscaled features\n",
    "\n",
    "# call the model\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = rf.predict_proba(X_train)\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = rf.predict_proba(X_test)\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model built in scaled features\n",
    "\n",
    "# call the model\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Train set\")\n",
    "pred = rf.predict_proba(X_train_scaled)\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = rf.predict_proba(X_test_scaled)\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Random Forests shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features. This model in particular, is over-fitting to the training set. So we need to do some work to remove the over-fitting. That exceeds the scope of this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "Let\u2019s evaluate the effect of feature scaling on AdaBoost. We will first build the model using unscaled variables and then the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adaboost on non-scaled features\n",
    "\n",
    "# call the model\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# train the model\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model performance\n",
    "print(\"Train set\")\n",
    "pred = ada.predict_proba(X_train)\n",
    "print(\"AdaBoost roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = ada.predict_proba(X_test)\n",
    "print(\"AdaBoost roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adaboost on scaled features\n",
    "\n",
    "# call the model\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# train the model\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate model performance\n",
    "print(\"Train set\")\n",
    "pred = ada.predict_proba(X_train_scaled)\n",
    "print(\"AdaBoost roc-auc: {}\".format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print(\"Test set\")\n",
    "pred = ada.predict_proba(X_test_scaled)\n",
    "print(\"AdaBoost roc-auc: {}\".format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, AdaBoost shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features\n",
    "\n",
    "Machine learning is like making a mixed fruit juice. If we want to get the best-mixed juice, we need to mix all fruit not by their size but based on their right proportion. We just need to remember apple and strawberry are not the same unless we make them similar in some context to compare their attribute. Similarly, in many machine learning algorithms, to bring all features in the same standing, we need to do scaling so that one significant number doesn\u2019t impact the model just because of their large magnitude. Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
