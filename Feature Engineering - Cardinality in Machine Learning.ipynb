{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Cardinality in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardinality refers to the number of possible values that a feature can assume. For example, the variable \u201cUS State\u201d is one that has 50 possible values. The binary features, of course, could only assume one of two values (0 or 1).\n",
    "\n",
    "The values of a categorical variable are selected from a group of categories, also called labels. For example, in the variable gender the categories or labels are male and female, whereas in the variable city the labels can be London, Manchester, Brighton and so on.\n",
    "\n",
    "Different categorical variables contain different number of labels or categories. The variable gender contains only 2 labels, but a variable like city or postcode, can contain a huge number of different labels.\n",
    "\n",
    "The number of different labels within a categorical variable is known as cardinality. A high number of labels within a variable is known as high cardinality.\n",
    "\n",
    "#### Are multiple labels in a categorical variable a problem?\n",
    "High cardinality may pose the following problems:\n",
    "\n",
    "Variables with too many labels tend to dominate over those with only a few labels, particularly in Tree based algorithms.\n",
    "A big number of labels within a variable may introduce noise with little, if any, information, therefore making machine learning models prone to over-fit.\n",
    "Some of the labels may only be present in the training data set, but not in the test set, therefore machine learning algorithms may over-fit to the training set.\n",
    "Contrarily, some labels may appear only in the test set, therefore leaving the machine learning algorithms unable to perform a calculation over the new (unseen) observation.\n",
    "In particular, tree methods can be biased towards variables with lots of labels (variables with high cardinality). Thus, their performance may be affected by high cardinality.\n",
    "\n",
    "Below we will see the effect of high cardinality of variables on the performance of different machine learning algorithms and how a quick fix to reduce the number of labels, without any sort of data insight, helps to boost the performance.\n",
    "\n",
    "#### In this Blog:\n",
    "We will:\n",
    "\n",
    "Learn how to quantify cardinality\n",
    "See examples of high and low cardinality variables\n",
    "Understand the effect of cardinality while preparing train and test sets\n",
    "See the effect of cardinality on Machine Learning Model performance\n",
    "We will use the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read the dataset into a dataframe and perform operations on it\n",
    "# to perform basic array operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "\n",
    "# to build machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# to evaluate the models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# to separate data into train and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read the titanic dataset using read_csv(). head() shows the first 5 rows of the dataframe. The categorical variables in this dataset are Name, Sex, Ticket, Cabin and Embarked.\n",
    "\n",
    "Note: Ticket and Cabin contain both letters and numbers, so they could be treated as Mixed Variables. For this demonstration, we will treat them as categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/titanic.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\u2019s inspect the cardinality of each categorical variable in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categories in the variable Name: {}\".format(len(data.Name.unique())))\n",
    "\n",
    "print(\"Number of categories in the variable Gender: {}\".format(len(data.Sex.unique())))\n",
    "\n",
    "print(\n",
    "    \"Number of categories in the variable Ticket: {}\".format(len(data.Ticket.unique()))\n",
    ")\n",
    "\n",
    "print(\"Number of categories in the variable Cabin: {}\".format(len(data.Cabin.unique())))\n",
    "\n",
    "print(\n",
    "    \"Number of categories in the variable Embarked: {}\".format(\n",
    "        len(data.Embarked.unique())\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Total number of passengers in the Titanic: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the variable Sex contains only 2 categories and Embarked contains 4 (low cardinality), the variables Ticket, Name and Cabin, as expected, contain a huge number of different labels (high cardinality).\n",
    "\n",
    "To demonstrate the effect of high cardinality in train and test sets and machine learning performance, we will work with the variable Cabin. We will create a new variable with reduced cardinality.\n",
    "\n",
    "We will begin by exploring the values in the variable Cabin. As we saw in the previous cell there are 148 unique values. We will display these values using unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Cabin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will reduce the cardinality of the variable. To do so, instead of using the entire cabin value, we will retain only the first letter in Cabin_reduced.\n",
    "\n",
    "Rationale: The first letter indicates the deck on which the cabin was located, and is therefore an indication of both social class status and proximity to the surface of the Titanic. Both are known to improve the probability of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's capture the first letter of Cabin\n",
    "data[\"Cabin_reduced\"] = data[\"Cabin\"].astype(str).str[0]\n",
    "\n",
    "data[[\"Cabin\", \"Cabin_reduced\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let\u2019s check the cardinality of Cabin_reduced. We reduced the number of different labels from 148 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categories in the variable Cabin: {}\".format(len(data.Cabin.unique())))\n",
    "\n",
    "print(\n",
    "    \"Number of categories in the variable Cabin_reduced: {}\".format(\n",
    "        len(data.Cabin_reduced.unique())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into training and testing set with the help of train_test_split(). use_col contains the variables of the feature space i.e. the variables which provide information necessary for prediction. Survived contains the values which have to be predicted. The test_size = 0.3 will keep 30% data for testing and 70% data will be used for training the model. random_state controls the shuffling applied to the data before applying the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\"Cabin\", \"Cabin_reduced\", \"Sex\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[use_cols], data[\"Survived\"], test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the previous cell the training set contains 623 rows and the test dataset contains 268 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High cardinality leads to uneven distribution of categories in train and test sets\n",
    "When a variable has high cardinality, often some categories land only in the training set, or only in the testing set. If present only in the training set, they may lead to over-fitting. If present only on the testing set, the machine learning algorithm will not know how to handle them, as it has not seen them during training.\n",
    "\n",
    "We will find the number of labels in Cabin which are present only in the training set and are not present in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_to_train_set = [\n",
    "    x for x in X_train.Cabin.unique() if x not in X_test.Cabin.unique()\n",
    "]\n",
    "\n",
    "len(unique_to_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 100 Cabins that are only present in the training set, and not in the testing set. Simillarly, we will compute the number of labels present only in the test set and not in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_to_test_set = [\n",
    "    x for x in X_test.Cabin.unique() if x not in X_train.Cabin.unique()\n",
    "]\n",
    "\n",
    "len(unique_to_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be overcomed by reducing the cardinality of the variable. Let\u2019s find out the number of labels present only in the training set for Cabin with reduced cardinality i.e. Cabin_reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_to_train_set = [\n",
    "    x\n",
    "    for x in X_train[\"Cabin_reduced\"].unique()\n",
    "    if x not in X_test[\"Cabin_reduced\"].unique()\n",
    "]\n",
    "\n",
    "len(unique_to_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the number of labels present only in the test set for Cabin with reduced cardinality i.e. Cabin_reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_to_test_set = [\n",
    "    x\n",
    "    for x in X_test[\"Cabin_reduced\"].unique()\n",
    "    if x not in X_train[\"Cabin_reduced\"].unique()\n",
    "]\n",
    "\n",
    "len(unique_to_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how by reducing the cardinality there is now only 1 label in the training set that is not present in the test set. And no labels in the test set which are not present in the training set.\n",
    "\n",
    "#### Effect of cardinality on Machine Learning Model Performance\n",
    "In order to evaluate the effect of categorical variables in machine learning models, we will quickly replace the categories by numbers. We will re-map Cabin to numbers so we can use it to train ML models.\n",
    "\n",
    "Note: This is neither the only nor the best way to encode categorical variables into numbers\n",
    "\n",
    "Here itertools is just used to display the first 100 elements in the newly created dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "cabin_dict = {k: i for i, k in enumerate(X_train[\"Cabin\"].unique(), 0)}\n",
    "print(dict(itertools.islice(cabin_dict.items(), 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will replace the labels in Cabin using the dictionary cabin_dict created above. The numerical values will be stored in Cabin_mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, \"Cabin_mapped\"] = X_train.loc[:, \"Cabin\"].map(cabin_dict)\n",
    "X_test.loc[:, \"Cabin_mapped\"] = X_test.loc[:, \"Cabin\"].map(cabin_dict)\n",
    "\n",
    "X_train[[\"Cabin_mapped\", \"Cabin\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that NaN takes the value 2 in the new variable, E17 takes the value 0, D33 takes the value 1, and so on. Now we will replace the letters in the Cabin_reduced variable with numbers following the same procedure as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create replace dictionary\n",
    "cabin_dict = {k: i for i, k in enumerate(X_train[\"Cabin_reduced\"].unique(), 0)}\n",
    "\n",
    "# replace labels by numbers with dictionary\n",
    "X_train.loc[:, \"Cabin_reduced\"] = X_train.loc[:, \"Cabin_reduced\"].map(cabin_dict)\n",
    "X_test.loc[:, \"Cabin_reduced\"] = X_test.loc[:, \"Cabin_reduced\"].map(cabin_dict)\n",
    "\n",
    "X_train[[\"Cabin_reduced\", \"Cabin\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that D33 and D26 correspond to the same number, 1, because we are capturing only the first letter. They both start with D.\n",
    "\n",
    "Now we wil map the categorical variable Sex to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, \"Sex\"] = X_train.loc[:, \"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "X_test.loc[:, \"Sex\"] = X_test.loc[:, \"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "X_train.Sex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will check if there are any missing values in these variables in the training as well as testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[\"Cabin_mapped\", \"Cabin_reduced\", \"Sex\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[[\"Cabin_mapped\", \"Cabin_reduced\", \"Sex\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n the test set, there are now 30 missing values for the highly cardinal variable Cabin_mapped. These were introduced while encoding the categories into numbers.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "Many categories exist only in the test set. Thus, when we created our encoding dictionary using only the train set, we did not generate a number to replace those labels present only in the test set. As a consequence, they were encoded as NaN. We will see in future notebooks how to tackle this problem. For now, we will fill those missing values with 0.\n",
    "\n",
    "Let\u2019s check the number of different categories in the encoded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.Cabin_mapped.unique()), len(X_train.Cabin_reduced.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can conclude that from the original 148 cabins in the dataset, only 121 are present in the training set. We also see how we reduced the number of different categories to just 9 in our previous step.\n",
    "\n",
    "Let\u2019s go ahead and evaluate the effect of labels on machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "We will build the model on data with high cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train[[\"Cabin_mapped\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = rf.predict_proba(X_train[[\"Cabin_mapped\", \"Sex\"]])\n",
    "pred_test = rf.predict_proba(X_test[[\"Cabin_mapped\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_train, pred_train[:, 1])))\n",
    "print(\"Test set\")\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the performance of the Random Forests on the training set is quite superior to its performance in the test set. This indicates that the model is over-fitting, which means that it does a great job at predicting the outcome on the dataset it was trained on, but it lacks the power to generalise the prediction for unseen data.\n",
    "\n",
    "Now we will build the model on data with low cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train[[\"Cabin_reduced\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = rf.predict_proba(X_train[[\"Cabin_reduced\", \"Sex\"]])\n",
    "pred_test = rf.predict_proba(X_test[[\"Cabin_reduced\", \"Sex\"]])\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_train, pred_train[:, 1])))\n",
    "print(\"Test set\")\n",
    "print(\"Random Forests roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the Random Forests no longer over-fitS to the training set. In addition, the model is much better at generalising the predictions.\n",
    "\n",
    "Note:- We can overcome the effect of high cardinality by adjusting the hyper-parameters of the random forests. That goes beyond the scope of this blog. Here, I want to show you that given a same model, with identical hyper-parameters, high cardinality may cause the model to over-fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "We will build the model on data with high cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# train the model\n",
    "ada.fit(X_train[[\"Cabin_mapped\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = ada.predict_proba(X_train[[\"Cabin_mapped\", \"Sex\"]])\n",
    "pred_test = ada.predict_proba(X_test[[\"Cabin_mapped\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\"Adaboost roc-auc: {}\".format(roc_auc_score(y_train, pred_train[:, 1])))\n",
    "print(\"Test set\")\n",
    "print(\"Adaboost roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the model on data with low cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# train the model\n",
    "ada.fit(X_train[[\"Cabin_reduced\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = ada.predict_proba(X_train[[\"Cabin_reduced\", \"Sex\"]])\n",
    "pred_test = ada.predict_proba(X_test[[\"Cabin_reduced\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\"Adaboost roc-auc: {}\".format(roc_auc_score(y_train, pred_train[:, 1])))\n",
    "print(\"Test set\")\n",
    "print(\"Adaboost roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adaboost model trained on the variable with high cardinality is also overfitting to the training set. Whereas the Adaboost trained on the low cardinal variable is not overfitting and therefore does a better job in generalising the predictions.\n",
    "\n",
    "In addition, building an AdaBoost on a model with less categories in Cabin, is a) simpler and b) should a different category in the test set appear, by taking just the front letter of cabin, the ML model will know how to handle it because it was seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "We will build the model on data with high cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "logit = LogisticRegression(random_state=44, solver=\"lbfgs\")\n",
    "\n",
    "# train the model\n",
    "logit.fit(X_train[[\"Cabin_mapped\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = logit.predict_proba(X_train[[\"Cabin_mapped\", \"Sex\"]])\n",
    "pred_test = logit.predict_proba(X_test[[\"Cabin_mapped\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\n",
    "    \"Logistic regression roc-auc: {}\".format(roc_auc_score(y_train, pred_train[:, 1]))\n",
    ")\n",
    "print(\"Test set\")\n",
    "print(\"Logistic regression roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the model on data with low cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "logit = LogisticRegression(random_state=44, solver=\"lbfgs\")\n",
    "\n",
    "# train the model\n",
    "logit.fit(X_train[[\"Cabin_reduced\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = logit.predict_proba(X_train[[\"Cabin_reduced\", \"Sex\"]])\n",
    "pred_test = logit.predict_proba(X_test[[\"Cabin_reduced\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\n",
    "    \"Logistic regression roc-auc: {}\".format(roc_auc_score(y_train, pred_train[:, 1]))\n",
    ")\n",
    "print(\"Test set\")\n",
    "print(\"Logistic regression roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw the same conclusion for Logistic Regression: reducing the cardinality improves the performance and generalisation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Classifier\n",
    "We will build the model on data with high cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "gbc = GradientBoostingClassifier(n_estimators=300, random_state=44)\n",
    "\n",
    "# train the model\n",
    "gbc.fit(X_train[[\"Cabin_mapped\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = gbc.predict_proba(X_train[[\"Cabin_mapped\", \"Sex\"]])\n",
    "pred_test = gbc.predict_proba(X_test[[\"Cabin_mapped\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\n",
    "    \"Gradient Boosted Trees roc-auc: {}\".format(\n",
    "        roc_auc_score(y_train, pred_train[:, 1])\n",
    "    )\n",
    ")\n",
    "print(\"Test set\")\n",
    "print(\n",
    "    \"Gradient Boosted Trees roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the model on data with low cardinality for cabin and then predict using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build on data with plenty of categories in Cabin variable\n",
    "\n",
    "# call the model\n",
    "gbc = GradientBoostingClassifier(n_estimators=300, random_state=44)\n",
    "\n",
    "# train the model\n",
    "gbc.fit(X_train[[\"Cabin_reduced\", \"Sex\"]], y_train)\n",
    "\n",
    "# make predictions on train and test set\n",
    "pred_train = gbc.predict_proba(X_train[[\"Cabin_reduced\", \"Sex\"]])\n",
    "pred_test = gbc.predict_proba(X_test[[\"Cabin_reduced\", \"Sex\"]].fillna(0))\n",
    "\n",
    "print(\"Train set\")\n",
    "print(\n",
    "    \"Gradient Boosted Trees roc-auc: {}\".format(\n",
    "        roc_auc_score(y_train, pred_train[:, 1])\n",
    "    )\n",
    ")\n",
    "print(\"Test set\")\n",
    "print(\n",
    "    \"Gradient Boosted Trees roc-auc: {}\".format(roc_auc_score(y_test, pred_test[:, 1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the algorithms give better performance when the cardinality of the variables is low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
