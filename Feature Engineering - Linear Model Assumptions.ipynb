{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Linear Model Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models make the following assumptions over the independent variables X, used to predict Y:\n",
    "\n",
    " - There is a linear relationship between X and the outcome Y\n",
    " - The independent variables X are normally distributed\n",
    " - There is no or little co-linearity among the independent variables\n",
    " - Homoscedasticity (homogeneity of variance)\n",
    "\n",
    "#### Examples of linear models are:\n",
    "\n",
    " - Linear and Logistic Regression\n",
    " - Linear Discriminant Analysis (LDA)\n",
    " - Principal Component Regressors\n",
    "\n",
    "#### Definitions:\n",
    " - Linear relationship describes a relationship between the independent variables X and the target Y that is given by: Y \u2248 \u03b20 + \u03b21X1 + \u03b22X2 + \u2026 + \u03b2nXn.\n",
    "\n",
    " - Normality means that every variable X follows a Gaussian distribution.\n",
    "\n",
    " - Multi-colinearity refers to the correlation of one independent variable with another. Variables should not be correlated.\n",
    "\n",
    " - Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the \u201cnoise\u201d or random disturbance in the relationship between the independent variables X and the dependent variable Y is the same across all the independent variables.\n",
    "\n",
    "Failure to meet one or more of the model assumptions may end up in a poor model performance. If the assumptions are not met, we can try a different machine learning model or transform the input variables so that they fulfill the assumptions.\n",
    "\n",
    "#### How can we evaluate if the assumptions are met by the variables?\n",
    " - Linear regression can be assessed by scatter-plots and residuals plots\n",
    " - Normal distribution can be assessed by Q-Q plots\n",
    " - Multi-colinearity can be assessed by correlation matrices\n",
    " - Homoscedasticity can be assessed by residuals plots\n",
    "\n",
    "#### What can we do if the assumptions are not met?\n",
    "Sometimes variable transformation can help the variables meet the model assumptions. We normally do 1 of 2 things:\n",
    "\n",
    " - Mathematical transformation of the variables\n",
    " - Discretisation\n",
    "\n",
    "#### In this blog\n",
    "We will learn the following things:\n",
    "\n",
    "Scatter plots and residual plots to visualise linear relationships\n",
    "Q-Q plots for normality\n",
    "Correlation matrices to determine co-linearity\n",
    "Residual plots for homoscedasticity\n",
    "We will compare the expected plots (how the plots should look like if the assumptions are met) obtained from simulated data, with the plots obtained from a toy dataset from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for the Q-Q plots\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "# the dataset for the demo\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# for linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# to evaluate the regression model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# to split and standarize the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the Boston House price dataset from sklearn. load_boston() loads and returns the boston house-prices dataset. We will create a pandas dataframe containing the independent variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dataset = load_boston()\n",
    "\n",
    "# create a dataframe with the independent variables\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "\n",
    "# add the target\n",
    "boston[\"MEDV\"] = boston_dataset.target\n",
    "\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_names returns the names of features i.e. a list of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = boston_dataset.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see the detailed description of the dataset. DESCR gives the full description of the dataset. Get familiar with the variables before continuing with the notebook. Our aim is to predict the median value of the houses i.e. the MEDV column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation data for the examples\n",
    "Now we will create a dataframe with variable x that follows a normal distribution and shows a linear relationship with y. This will provide the expected plots i.e. how the plots should look like if the assumptions are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(29)  # for reproducibility\n",
    "\n",
    "n = 200\n",
    "x = np.random.randn(n)\n",
    "y = x * 10 + np.random.randn(n) * 2\n",
    "\n",
    "toy_df = pd.DataFrame([x, y]).T\n",
    "toy_df.columns = [\"x\", \"y\"]\n",
    "toy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Assumption\n",
    "We evaluate linear assumption with scatter plots and residual plots. Scatter plots are used to plot the change in the dependent variable y with the independent variable x.\n",
    "\n",
    "#### Scatter plots\n",
    "Now we will see how the scatter plot looks like for the simulated data. This is how the plot should look like when there is a linear relationship. sns.lmplot() method is used to draw a scatter plot onto a FacetGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order 1 indicates that we want seaborn to estimate a linear model (the line in the plot below) between x and y\n",
    "sns.lmplot(x=\"x\", y=\"y\", data=toy_df, order=1)\n",
    "\n",
    "plt.ylabel(\"Target\")\n",
    "plt.xlabel(\"Independent variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will draw a scatter plot for the boston house price dataset. We will plot LSAST (% lower status of the population) vs MEDV (Median value of owner-occupied homes in $1000\u2019s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"LSTAT\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between LSTAT and MEDV is linear, apart from a few values around the minimal values of LSTAT i.e. towards the top left hand side of the plot.\n",
    "\n",
    "Now we plot RM (average number of rooms per dwelling) vs MEDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"RM\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is not so clear whether the relationship is linear. It does seem so around the center of the plot, but there are a lot of dots that do not adjust to the line.\n",
    "\n",
    "Now we plot CRIM (per capita crime rate by town) vs MEDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"CRIM\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between CRIM and MEDV is clearly not linear. A transformation of CRIM may helps improve the linear relationship. We will apply log transformation to CRIM and draw the plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston[\"log_crim\"] = np.log(boston[\"CRIM\"])\n",
    "\n",
    "# plot the transformed CRIM variable vs MEDV\n",
    "sns.lmplot(x=\"log_crim\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation certainly improved the linear fit between CRIM and MEDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the added log transformed variable we don't need it for the rest of the demo\n",
    "\n",
    "boston.drop(labels=\"log_crim\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing linear relationship by examining the residuals (errors)\n",
    "Another thing that we can do to determine whether there is a linear relationship between the variable and the target is to evaluate the distribution of the errors, or the residuals. The residuals refer to the difference between the predictions and the real value of the target. It is performed as follows:\n",
    "\n",
    "1) Make a linear regression model using the desired variables (X)\n",
    "\n",
    "2) Obtain the predictions\n",
    "\n",
    "3) Determine the error (True house price \u2013 predicted house price)\n",
    "\n",
    "4) Observe the distribution of the error.\n",
    "\n",
    "If the house price, in this case MEDV, is linearly explained by the variables we are evaluating, then the error should be random noise, and should typically follow a normal distribution centered at 0. We expect to see the error terms for each observation lying around 0.\n",
    "\n",
    "We will do this first, for the simulated data, to become familiar with how the plots should look like. Then we will do the same for LSTAT and then, we will transform LSTAT to see how transformation affects the residuals and the linear fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED DATA\n",
    "\n",
    "# step 1: make a linear model\n",
    "# call the linear model from sklearn\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(toy_df[\"x\"].to_frame(), toy_df[\"y\"])\n",
    "\n",
    "# step 2: obtain the predictions\n",
    "# make the predictions\n",
    "pred = linreg.predict(toy_df[\"x\"].to_frame())\n",
    "\n",
    "# step 3: calculate the residuals\n",
    "error = toy_df[\"y\"] - pred\n",
    "\n",
    "# plot predicted vs real\n",
    "plt.scatter(x=pred, y=toy_df[\"y\"])\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Real value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model makes good predictions. The predictions are quite aligned with the real value of the target.\n",
    "\n",
    "Now we will visualize the distribution of errors. If the relationship is linear, the noise should be random, centered around zero, and should follow a normal distribution. We plot the error terms vs the independent variable x. Error values should be around 0 and homogeneously distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: observe the distribution of the errors\n",
    "\n",
    "plt.scatter(y=error, x=toy_df[\"x\"])\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Independent variable x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the errors are distributed around 0.\n",
    "\n",
    "Now we will plot a histogram of the residuals (errors). They should follow a gaussian distribution centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(error, bins=30)\n",
    "plt.xlabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors adopt a Gaussian distribution and it is centered around 0. So it meets the assumptions, as expected.\n",
    "\n",
    "Let\u2019s do the same for LSTAT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: call the linear model from sklearn\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(boston[\"LSTAT\"].to_frame(), boston[\"MEDV\"])\n",
    "\n",
    "# step 2: make the predictions\n",
    "pred = linreg.predict(boston[\"LSTAT\"].to_frame())\n",
    "\n",
    "# step 3: calculate the residuals\n",
    "error = boston[\"MEDV\"] - pred\n",
    "\n",
    "# plot predicted vs real\n",
    "plt.scatter(x=pred, y=boston[\"MEDV\"])\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a relatively good fit for most of the predictions, but the model does not predict very well towards the highest house prices. For high house prices, the model under-estimates the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: observe the distribution of the errors\n",
    "\n",
    "plt.scatter(y=error, x=boston[\"LSTAT\"])\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not really centered around zero. And the errors are not homogeneously distributed across the values of LSTAT. Low and high values of LSTAT show higher errors.\n",
    "\n",
    "The relationship could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the residuals\n",
    "# should follow a gaussian distribution\n",
    "\n",
    "sns.distplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not centered around zero, and the distribution is not totally Gaussian. There is a peak at around 20. Can we improve the fit by transforming LSTAT? To find out let\u2019s repeat the exercise by fitting the model to log transformed LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: call the linear model from sklearn\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(np.log(boston[\"LSTAT\"]).to_frame(), boston[\"MEDV\"])\n",
    "\n",
    "# staep 2: make the predictions\n",
    "pred = linreg.predict(np.log(boston[\"LSTAT\"]).to_frame())\n",
    "\n",
    "# step 3: calculate the residuals\n",
    "error = boston[\"MEDV\"] - pred\n",
    "\n",
    "# plot predicted vs real\n",
    "plt.scatter(x=pred, y=boston[\"MEDV\"])\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions seem a bit better compared to the non-transformed variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: observe the distribution of the errors\n",
    "\n",
    "plt.scatter(y=error, x=boston[\"LSTAT\"])\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are more centered around zero and more homogeneously distributed across the values of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the residuals\n",
    "# should follow a gaussian distribution\n",
    "\n",
    "sns.distplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram looks more Gaussian, and the peak towards 20 has now disappeared. We can see how a variable transformation improved the fit and helped meet the linear model assumption of linearity.\n",
    "\n",
    "Go ahead and try this in the variables RM and CRIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicolinearity\n",
    "To determine co-linearity, we evaluate the correlation of all the independent variables in the dataframe. We will calculate the correlations using pandas corr() and round off the values to 2 decimals. We will plot the correlation matrix using heatmaps(). A heatmap is a graphical representation of data in which data values are represented as colors. They are typically used to visualize correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = boston[features].corr().round(2)\n",
    "figure = plt.figure(figsize=(12, 12))\n",
    "# annot = True to print the correlation values inside the squares\n",
    "sns.heatmap(data=correlation_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the x and y axis of the heatmap we have the variables of the boston house dataframe. Within each square, the correlation value between those 2 variables is indicated. For example, for LSTAT vs CRIM at the bottom left of the heatmap, we see a correlation of 0.46. These 2 variables are not highly correlated.\n",
    "\n",
    "Instead, for the variables RAD and TAX, the correlation is 0.91. These variables are highly correlated. The same is true for the variables NOX and DIS, which show a correlation value of -0.71.\n",
    "\n",
    "Let\u2019s see how they look in a scatter plot. We will plot the correlation between RAD (index of accessibility to radial highways) and TAX (full-value property-tax rate per $10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"RAD\", y=\"TAX\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simillary for NOX (itric oxides concentration (parts per 10 million)) and DIS (weighted distances to five Boston employment centres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"NOX\", y=\"DIS\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation, or co-linearity between NOX and DIS, is quite obvious in the above scatter plot. So these variables are violating the assumption of no multi co-linearity.\n",
    "\n",
    "We would remove 1 of the 2 from the dataset before training the linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality\n",
    "We evaluate normality using histograms and Q-Q plots. Let\u2019s begin with histograms. If the variable is normally distributed, we should observe the typical Gaussian bell shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "First we will plot the histogram of the simulated independent variable x which we know follows a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(toy_df[\"x\"], bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the histogram of the variable RM (average number of rooms per dwelling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(boston[\"RM\"], bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable seems to follow a Normal distribution hence it meets the assumption.\n",
    "\n",
    "We will plot a histogram for the variable LSTAT (% lower status of the population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(boston[\"LSTAT\"], bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTAT is skewed. Let\u2019s see if a transformation fixes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the log-transformed LSTAT for comparison\n",
    "sns.distplot(np.log(boston[\"LSTAT\"]), bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is less skewed, but not totally normal either. We could go ahead and try other transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Q plots\n",
    "In a Q-Q plot, the quantiles of the variable are plotted on the vertical axis (y), and the quantiles of a specified probability distribution (Gaussian distribution) are indicated on the horizontal axis (x). The plot consists of a series of points that show the relationship between the quantiles of the real data and the quantiles of the specified probability distribution. If the values of a variable perfectly match the specified probability distribution (i.e. the normal distribution), the points on the graph will form a 45 degree line.\n",
    "\n",
    "Let\u2019s plot the Q-Q plot for the simualted data. The dots should adjust to the 45 degree line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(toy_df[\"x\"], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they do. This is how a normal distribution looks like in a Q-Q plot. Let\u2019s do the same for RM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(boston[\"RM\"], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the points adjust to the 45 degree line. However, the values at both ends of the distribution deviate from the line. This indicates that the distribution of RM is not perfectly Gaussian.\n",
    "\n",
    "Let\u2019s plot for LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(boston[\"LSTAT\"], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the observations lie on the 45 degree red line following the expected quantiles of the theoretical Gaussian distribution, particularly towards the center of the plot. Some observations at the lower and upper end of the value range depart from the red line, which indicates that the variable LSTAT is not normally distributed, as we rightly see in the histogram.\n",
    "\n",
    "Let\u2019s see if a transformation improves the normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now for the log transformed LSTAT\n",
    "stats.probplot(np.log(boston[\"LSTAT\"]), dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after the transformation, the quantiles are more aligned over the 45 degree line with the theoretical quantiles of the Gaussian distribution.\n",
    "\n",
    "Just for comparison, let\u2019s go ahead and plot CRIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(boston[\"CRIM\"], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let\u2019s see if a transformation improves the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(np.log(boston[\"CRIM\"]), dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this case, the transformation improved the fit, but the transformed distribution is not Gaussian. You could try with a different transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homocedasticity\n",
    "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the \u201cnoise\u201d or random disturbance in the relationship between the independent variables X and the dependent variable Y is the same across all the independent variables.\n",
    "\n",
    "The way to identify if the variables are homoscedastic, is to make a linear model with all the independent variables involved, calculate the residuals, and plot the residuals vs each one of the independent variables. If the distribution of the residuals is homogeneous across the variable values, then the variables are homoscedastic.\n",
    "\n",
    "There are other tests for homoscedasticity:\n",
    "\n",
    " - Residual plot\n",
    " - Levene\u2019s test\n",
    " - Barlett\u2019s test\n",
    " - Goldfeld-Quandt Test\n",
    " \n",
    "For this demo we will focus on residual plot analysis.\n",
    "\n",
    "To train and evaluate the model we will split the data into training and testing set with the help of train_test_split(). We will use the variables RM, LSTAT and CRIM as the feature space and MEDV as the target. The test_size = 0.3 will keep 30% data for testing and 70% data will be used for training the model. random_state controls the shuffling applied to the data before applying the split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston[[\"RM\", \"LSTAT\", \"CRIM\"]], boston[\"MEDV\"], test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will scale the features. StandardScaler() standardizes the features by removing the mean and scaling to unit variance\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x \u2013 u) / s\n",
    "\n",
    "where u is the mean of the training samples and s is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler(copy=True, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# make predictions on the train set and calculate the mean squared error\n",
    "print(\"Train set\")\n",
    "pred = linreg.predict(scaler.transform(X_train))\n",
    "print(\"Linear Regression mse: {}\".format(mean_squared_error(y_train, pred)))\n",
    "\n",
    "# make predictions on the test set and calculate the mean squared error\n",
    "print(\"Test set\")\n",
    "pred = linreg.predict(scaler.transform(X_test))\n",
    "print(\"Linear Regression mse: {}\".format(mean_squared_error(y_test, pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will calculate the residuals and plot the residuals vs LSTAT which is one of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residuals\n",
    "error = y_test - pred\n",
    "\n",
    "# plot the residuals vs LSTAT\n",
    "plt.scatter(x=X_test[\"LSTAT\"], y=error)\n",
    "plt.xlabel(\"LSTAT\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals seem fairly homogeneously distributed across the values of LSTAT.\n",
    "\n",
    "Let\u2019s plot the residuals vs RM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_test[\"RM\"], y=error)\n",
    "plt.xlabel(\"RM\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this variable, the residuals do not seem to be homogeneously distributed across the values of RM. In fact, low and high values of RM show higher error terms.\n",
    "\n",
    "Now we will plot residuals vs CRIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_test[\"CRIM\"], y=error)\n",
    "plt.xlabel(\"CRIM\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals for this particular variable are very skewed.\n",
    "\n",
    "We will plot a histogram for error. The distribution of the residuals is fairly normal, but not quite, with more high values than expected towards the right end of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the yellobricks library. It is a library for visualisation of machine learning model outcomes.\n",
    "\n",
    "To install the library you can run the following command-\n",
    "\n",
    "##### conda install --name tensorflow20 -c conda-forge yellowbrick\n",
    "\n",
    "yellowbricks allows you to visualise the residuals of the models after fitting a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "\n",
    "visualizer.fit(scaler.transform(X_train), y_train)  # Fit the training data to the model\n",
    "visualizer.score(\n",
    "    scaler.transform(X_test), y_test\n",
    ")  # Evaluate the model on the test data\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the plot that the residuals are not homogeneously distributed across the predicted value and are not centered around zero either.\n",
    "\n",
    "Let\u2019s see if transformation of the variables CRIM and LSTAT helps improve the fit and the homoscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform the variables\n",
    "boston[\"LSTAT\"] = np.log(boston[\"LSTAT\"])\n",
    "boston[\"CRIM\"] = np.log(boston[\"CRIM\"])\n",
    "boston[\"RM\"] = np.log(boston[\"RM\"])\n",
    "\n",
    "# let's separate into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston[[\"RM\", \"LSTAT\", \"CRIM\"]], boston[\"MEDV\"], test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scale the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler(copy=True, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "# call the model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# make predictions and calculate the mean squared error over the train set\n",
    "print(\"Train set\")\n",
    "pred = linreg.predict(scaler.transform(X_train))\n",
    "print(\"Linear Regression mse: {}\".format(mean_squared_error(y_train, pred)))\n",
    "\n",
    "# make predictions and calculate the mean squared error over the test set\n",
    "print(\"Test set\")\n",
    "pred = linreg.predict(scaler.transform(X_test))\n",
    "print(\"Linear Regression mse: {}\".format(mean_squared_error(y_test, pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare these squared errors with the ones obtained using the non-transformed data, you can see that transformation improved the fit, as the mean squared errors for both train and test sets are smaller after using transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residuals\n",
    "error = y_test - pred\n",
    "\n",
    "# residuals plot vs LSTAT\n",
    "plt.scatter(x=X_test[\"LSTAT\"], y=error)\n",
    "plt.xlabel(\"LSTAT\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values seem homogeneously distributed across values of LSTAT and centered around zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals plot vs RM\n",
    "plt.scatter(x=X_test[\"RM\"], y=error)\n",
    "plt.xlabel(\"RM\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation improved the spread of the residuals across the values of RM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals plot vs CRIM\n",
    "plt.scatter(x=X_test[\"CRIM\"], y=error)\n",
    "plt.xlabel(\"CRIM\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation helped distribute the residuals across the values of CRIM.\n",
    "\n",
    "Now we will plot a histogram for error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the residuals is more Gaussian looking now. There is still a few higher than expected residuals towards the right of the distribution, but leaving those apart, the distribution seems less skewed than with the non-transformed data.\n",
    "\n",
    "Now let\u2019s plot the residuals using yellowbricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "\n",
    "visualizer.fit(scaler.transform(X_train), y_train)  # Fit the training data to the model\n",
    "visualizer.score(\n",
    "    scaler.transform(X_test), y_test\n",
    ")  # Evaluate the model on the test data\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are more homogeneously distributed and centered around 0.\n",
    "\n",
    "The errors are more homogeneously distributed and centered around 0.\n",
    "\n",
    "Look at the R square values in the yellowbricks residual plots. Compare the values for the models utilising the transformed and non-transformed data. We can see how transforming the data, improved the fit (Test R square of 0.65 for transformed data vs 0.6 for non-transformed data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
